---
title: "hw5-Akshit-Jain"
author: "Akshit Jain"
date: "11/6/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(modelr)
library(purrr)
library(mlbench)
```

## Part A

### Problem 1
I choose the miniposter made by Manaswini Nagaraj: This dataset contains attributes of patients who suffer from abnormalities in the pelvic and lumbar areas. Source: https://www.kaggle.com/uciml/biomechanical-features-of-orthopedic-patients

```{r f1, echo = FALSE}
load_data <- function(df) {
  head(df, 10)
}
```
```{r}
df <- read_csv('column_2C_weka.csv', col_types = cols())
load_data(df)
```

```{r f1, eval = FALSE}
```

### Problem 2

```{r f2, echo = FALSE}
make_plots <- function(df) {
  plt1 <- df %>% 
    mutate(class = case_when((class == 'Abnormal') ~ 1,
                             (class == 'Normal') ~ 0)) %>%
    ggplot() + 
    geom_boxplot(mapping = aes(x=as.factor(class),
                               y=pelvic_incidence,
                               fill=as.factor(class)))
             
  print(plt1)
  
  plt2 <- df %>% 
    mutate(class = case_when((class == 'Abnormal') ~ 1,
                             (class == 'Normal') ~ 0)) %>%
    ggplot() + 
    geom_point(mapping = aes(x=pelvic_radius,
                             y=degree_spondylolisthesis, 
                             color=as.factor(class)))
  print(plt2)
                                           

}
```
```{r}
make_plots(df)
```

```{r f2, eval = FALSE}
```

## Part B

### Problem 3

```{r f3, echo = FALSE}
cross_validation <- function(model, df, k) {
  set.seed(1)
  data_cv <- crossv_kfold(df, k)
  data_cv <- data_cv %>% 
    mutate(fit = map(train, ~ model, data_cv = .)) %>%
    mutate(rmse_train = map2_dbl(fit, train, ~ rmse(.x, .y)),
           rmse_test = map2_dbl(fit, test, ~ rmse(.x, .y)))
  
  return(mean(data_cv$rmse_test))
}
```

### Problem 4

```{r f4, echo = FALSE}
model_selection <- function(df) {
  # proposed model
  proposed_model <- lm(log2(crim) ~ log2(dis) + medv, data = df)
  proposed_rmse = cross_validation(proposed_model, df, 5)
  print(paste('RMSE of the proposed model from Homework 4:', 
              proposed_rmse))
  
  # model selection
  fit_age <- lm(log2(crim) ~ age, data = df)
  fit_dis <- lm(log2(crim) ~ log2(dis), data = df)
  fit_rad <- lm(log2(crim) ~ rad, data = df)
  fit_tax <- lm(log2(crim) ~ tax, data = df)
  fit_ptratio <- lm(log2(crim) ~ ptratio, data = df)
  
  print(paste('fit_age rmse:', cross_validation(fit_age, df, 5)))
  print(paste('fit_dis rmse:', cross_validation(fit_dis, df, 5)))
  print(paste('fit_rad rmse:', cross_validation(fit_rad, df, 5)))
  print(paste('fit_tax rmse:', cross_validation(fit_tax, df, 5)))
  print(paste('fit_ptratio rmse:', cross_validation(fit_ptratio, df, 5)))
  
  print('----------------------------')
  print('rad has the smallest rmse, hence it will be our first predictor for the model')
  print('----------------------------')
  
  fit_age2 <- lm(log2(crim) ~ rad + age, data = df)
  fit_dis2 <- lm(log2(crim) ~ rad + log2(dis), data = df)
  fit_tax2 <- lm(log2(crim) ~ rad + tax, data = df)
  fit_ptratio2 <- lm(log2(crim) ~ rad + ptratio, data = df)
   
  print(paste('fit_age2 rmse:', cross_validation(fit_age2, df, 5)))
  print(paste('fit_dis2 rmse:', cross_validation(fit_dis2, df, 5)))
  print(paste('fit_tax2 rmse:', cross_validation(fit_tax2, df, 5)))
  print(paste('fit_ptratio2 rmse:', cross_validation(fit_ptratio2, df, 5)))
  
  print('----------------------------')
  print('log2(dis) will be our second predictor for the model')
  print('----------------------------')
  
  fit_age3 <- lm(log2(crim) ~ rad + log2(dis) + age, data = df)
  fit_tax3 <- lm(log2(crim) ~ rad + log2(dis) + tax, data = df)
  fit_ptratio3 <- lm(log2(crim) ~ rad + log2(dis) + ptratio, data = df)
  
  print(paste('fit_age3 rmse:', cross_validation(fit_age3, df, 5)))
  print(paste('fit_tax3 rmse:', cross_validation(fit_tax3, df, 5)))
  print(paste('fit_ptratio3 rmse:', cross_validation(fit_ptratio3, df, 5)))
  
  print('----------------------------')
  print('age will be our third predictor for the model')
  print('----------------------------')
  
  fit_tax4 <- lm(log2(crim) ~ rad + log2(dis) + age + tax, data = df)
  fit_ptratio4 <- lm(log2(crim) ~ rad + log2(dis) + age + ptratio, data = df)
  
  print(paste('fit_tax4 rmse:', cross_validation(fit_tax4, df, 5)))
  print(paste('fit_ptratio4 rmse:', cross_validation(fit_ptratio4, df, 5)))
  
  print('----------------------------')
  print('tax will be our fourth predictor for the model')
  print('----------------------------')
  
  fit_ptratio5<- lm(log2(crim) ~ rad + log2(dis) + age + tax + ptratio, data = df)
  print(paste('fit_ptratio5 rmse:', cross_validation(fit_ptratio5, df, 5)))
  
  print('----------------------------')
  print('tax and patratio, have little contribution in reducing the rmse.')
  print('----------------------------')
  
  # plot rmse at each step of variable selection
  fits_rmse <- tibble(nvar = 1:5, rmse = c(cross_validation(fit_rad, df, 5),
                                           cross_validation(fit_dis2, df, 5),
                                           cross_validation(fit_age3, df, 5),
                                           cross_validation(fit_tax4, df, 5),
                                           cross_validation(fit_ptratio5, df, 5)))
  fits_rmse %>% ggplot() + geom_line(mapping = aes(x = nvar, y = rmse)) + 
    ggtitle('How does the model improve as we add variables?')
}
```
```{r}
data("BostonHousing")
model_selection(BostonHousing)
```
```{r f4, eval = FALSE}
```
\textbf{Observation:} The most predictive model using these variable was **fit_ptratio5 <- lm(log2(crim) ~ rad + log2(dis) + age + tax + ptratio, data = df)** with an average testing rmse of 1.2325, which is smaller than 1.9293 of my proposed model from Homework 4.

## Problem 5
Yes, we can report the final cross-validated RMSE for the “best” model that we found in Problem 4 as a good measure of the RMSE we could expect on new data. Cross-validation is a good technique to test a model on its predictive performance. While a model may minimize the Root Mean Squared Error on the training data, it can be optimistic in its predictive error. The partitions used in cross-validation help to simulate an independent dataset (i.e. test data, something that the model has not seen before) and get a better assessment of a model’s predictive performance. Therefore, it is safe to say that we can report the final cross-validated RMSE as a good measure of the RMSE of new data.
